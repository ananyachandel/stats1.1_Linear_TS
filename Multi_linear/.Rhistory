cellular.fit3 <- update(cellular.fit2, ~ . - income:popul.med.age:popul.urban)
summary(cellular.fit3)
# Using step process
cellular.fit4 <- step(cellular.fit3)
summary(cellular.fit4)
# Manually eliminating insignificant variable (concept of parsimony)
cellular.fit5 <- update(cellular.fit4, ~ . - popul.med.age:popul.in.thous)
summary(cellular.fit5)
# Using step process
cellular.fit6 <- step(cellular.fit5)
summary(cellular.fit6)
# Manually eliminating insignificant variable (concept of parsimony)
cellular.fit7 <- update(cellular.fit6, ~ . - income:popul.med.age)
summary(cellular.fit7)
# Using step process
cellular.fit8 <- step(cellular.fit7)
summary(cellular.fit8)
# Manually eliminating insignificant variable (concept of parsimony)
cellular.fit9 <- update(cellular.fit8, ~ . - popul.in.thous )
summary(cellular.fit9)
# checking plot
par(mfrow =c(2,2))
plot(cellular.fit9)
# checking statstics
# F-statistic:
# Gauss– Markov Assumptions - Errors have constant variance, which is known as homoscedasticity
ncvTest(cellular.fit9)
# There is no autocorrelation between errors ::
durbinWatsonTest(cellular.fit9)
# Predictor variables must be independent of the error term (Omitted variable bias!) ::
# We assume that our errors are normally distributed :: From Q-Q plot and histogram
par(mfrow = c(1,1))
hist(cellular.fit9$residuals) # is near to normal distribution
# We assume there is no multicollinearity between predictors ::
vif(cellular.fit9)
# The third assumption we make is that we have no influential data points ::
cooks.distance(cellular.fit9)
influencePlot(cellular.fit9)
# checking plot
par(mfrow =c(2,2))
plot(cellular.fit9)
# dropping influential/ problamatic variables i.e. 76,78,90
cellular <- cellular[- c(76,78,90),]
# setting the index of the rows in seq
row.names(cellular) <- 1:159
# again the cellular.fit9
cellular.fit9 <- lm(formula = cellular ~ popul.med.age + popul.urban + popul.med.age:popul.urban,
data = cellular)
summary(cellular.fit9)
# checking plot
par(mfrow =c(2,2))
plot(cellular.fit9)
# checking statstics
# F-statistic:
# Gauss– Markov Assumptions - Errors have constant variance, which is known as homoscedasticity
ncvTest(cellular.fit9)
# There is no autocorrelation between errors ::
durbinWatsonTest(cellular.fit9)
# Predictor variables must be independent of the error term (Omitted variable bias!) ::
# We assume that our errors are normally distributed :: From Q-Q plot and histogram
par(mfrow = c(1,1))
hist(cellular.fit9$residuals) # is near to normal distribution
# We assume there is no multicollinearity between predictors ::
vif(cellular.fit9)
# The third assumption we make is that we have no influential data points ::
cooks.distance(cellular.fit9)
influencePlot(cellular.fit9)
setwd("/Users/sobil/Downloads/Stats_Data_sets/")
remove(list = ls())
#
cellular <- read.csv("Cellular.csv", stringsAsFactors = FALSE)
cellular <- cellular[,-c(2,4)]
#
other <- read.csv("Gross national income per capita (PPP int. $).csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income")
#
other <- read.csv("Population_medain_age.csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income","popul.med.age")
#
other <- read.csv("Population_Urban.csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income","popul.med.age","popul.urban")
#
other <- read.csv("PopulationInThousand.csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income","popul.med.age","popul.urban","popul.in.thous")
cellular <- cellular[,-1]
# checking distribution & outliers
summary(cellular)
boxplot(cellular$cellular) # no outliers
hist(cellular$cellular) # normally distributed
# checking correlation
library(psych)
psych::pairs.panels(cellular)
# Primary model with intercept only
cellular.fit1 <- lm(cellular ~ . , data = cellular)
summary(cellular.fit1)
# Using step forward
cellular.fit2 <- step(cellular.fit1, scope = . ~ .^2, direction = "forward")
# Using step forward
cellular.fit2 <- step(cellular.fit1, scope = . ~ .^2 ~ .^3, direction = "forward")
summary(cellular.fit2)
# Manually eliminating insignificant variable (concept of parsimony)
cellular.fit3 <- update(cellular.fit2, ~ . - income:popul.med.age)
summary(cellular.fit3)
# Using step process
cellular.fit4 <- step(cellular.fit3)
summary(cellular.fit4)
# Manually eliminating insignificant variable (concept of parsimony)
cellular.fit5 <- update(cellular.fit4, ~ . - popul.in.thous)
summary(cellular.fit5)
setwd("/Users/sobil/Downloads/Stats_Data_sets/")
remove(list = ls())
#
cellular <- read.csv("Cellular.csv", stringsAsFactors = FALSE)
cellular <- cellular[,-c(2,4)]
#
other <- read.csv("Gross national income per capita (PPP int. $).csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income")
#
other <- read.csv("Population_medain_age.csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income","popul.med.age")
#
other <- read.csv("Population_Urban.csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income","popul.med.age","popul.urban")
#
other <- read.csv("PopulationInThousand.csv", stringsAsFactors = FALSE)
other <- other[,-c(2,4)]
cellular <- merge(cellular, other, by = "Country.or.Area")
colnames(cellular) <- c("Country.or.Area","cellular","income","popul.med.age","popul.urban","popul.in.thous")
cellular <- cellular[,-1]
# checking distribution & outliers
summary(cellular)
boxplot(cellular$cellular) # no outliers
hist(cellular$cellular) # normally distributed
# checking correlation
library(psych)
psych::pairs.panels(cellular)
# Concept of PARSIMONY
# checking the best fit predictors
library(leaps)
library(car)
bstFits1 <- regsubsets(cellular ~
income*popul.med.age*popul.urban*popul.in.thous +
I(income^2) + I(popul.med.age^2) + I(popul.urban^2) + I(popul.in.thous^2),
data = cellular, nbest = 1, nvmax = 4)
par(mfrow = c(1,1))
subsets(bstFits1, statistic = "adjr2")
setwd("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Multi_linear")
remove(list = ls())
#
cellular <- read.csv("Cellular.csv", stringsAsFactors = FALSE)
remove(list = ls())
gdp <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Time-Series/GDP_Ireland.csv")
str(gdp)
summary(gdp)
View(gdp)
sort(gdp)
sort(gdp$Year)
order(gdp)
gdp <- order(gdp)
gdp <- gdp(order(gdp),)
gdp <- gdp[order(gdp),]
# reading the gdp file
gdp <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Time-Series/GDP_Ireland.csv")
gdp <- gdp[sort(range(49,1,1),]
View(gdp)
gdp[order(Year),]
gdp[order("Year"),]
gdp <- gdp[order("Year"),]
# reading the gdp file
gdp <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Time-Series/GDP_Ireland.csv")
gdp <- gdp[order(gdp$Year),]
View(gdp)
row.names(gdp) <- 1:49
View(gdp)
remove(list = ls())
# reading the gdp file
data <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Time-Series/GDP_Ireland.csv")
str(data)
summary(data)
# sorting the data
data <- data[order(data$Year),]
row.names(gdp) <- 1:49
row.names(data) <- 1:49
View(data)
View(data)
# creating time series data
gdp <- ts(data = data$Gross.Domestic.Product..GDP., start = 1970, end = 2018, frequency = 1)
gdp
autoplot(gdp)
plot(gdp)
start(gdp)
stop(gdp)
end(gdp)
# creating time series data and analysing
gdp <- ts(data = data$Gross.Domestic.Product..GDP., start = 1970, end = 2018, frequency = 1)
plot(gdp)
start(gdp)
end(gdp)
library(fpp2)
plot(ma(gdp,3))
# smotthing the plot for checking Moving average
plot(ma(gdp,1))
plot(gdp)
# checking the plots
plot(gdp, main = "Normal")
par(mfrow = c(2,2))
# checking the plots
plot(gdp, main = "Normal")
# smotthing the plot for checking Moving average
plot(ma(gdp,1), main = "q = 1")
plot(ma(gdp,3), main = "q = 3")
plot(ma(gdp,5), main = "q = 5")
# checking the plots
plot(gdp, main = "Normal")
library(fpp2)
# smotthing the plot for checking Moving average
plot(ma(gdp,3), main = "q = 3")
plot(ma(gdp,5), main = "q = 5")
plot(ma(gdp,7), main = "q = 7")
fit.decmp <- decompose(gdp, type = "additive")
# Models
# Simple Exponential Model -
gdp.ses <- ses(gdp)
# Models
# Simple Exponential Model -
par(mfrow = c(2,2))
gdp.ses <- ses(gdp)
gdp.ses
round(accuracy(gdp.ses),3)
summary(data)
# converting the predictor to billion figures
data$Gross.Domestic.Product..GDP. <- data$Gross.Domestic.Product..GDP./1000000000
summary(data)
summary(data)
remove(list = ls())
# reading the gdp file
data <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Time-Series/GDP_Ireland.csv")
str(data)
summary(data)
# converting the predictor to billion figures
data$Gross.Domestic.Product..GDP. <- data$Gross.Domestic.Product..GDP./1000000000
summary(data)
# sorting the data
data <- data[order(data$Year),]
row.names(data) <- 1:49
# creating time series data and analysing
gdp <- ts(data = data$Gross.Domestic.Product..GDP., start = 1970, end = 2018, frequency = 1)
par(mfrow = c(2,2))
# checking the plots
plot(gdp, main = "Normal")
library(fpp2)
# smotthing the plot for checking Moving average
plot(ma(gdp,3), main = "q = 3")
plot(ma(gdp,5), main = "q = 5")
plot(ma(gdp,7), main = "q = 7")
# Models
# Simple Exponential Model -
par(mfrow = c(2,2))
# Models
# Simple Exponential Model -
par(mfrow = c(1,1))
gdp.ses <- ses(gdp)
gdp.ses
round(accuracy(gdp.ses),3)
# Models
# Simple Exponential Model 1 -
par(mfrow = c(1,1))
gdp.1.ses <- ses(gdp)
gdp.1.ses
round(accuracy(gdp.1.ses),3)
autoplot(gdp.1.ses)
gdp.1.ses <- ses(gdp, h = 3)
gdp.1.ses
round(accuracy(gdp.1.ses),3)
autoplot(gdp.1.ses) + autolayer(fitted(gdp.1.ses), series = "Fitted")
gdp.1.ets <- ets(gdp,model = "ANN")
gdp.1.ets
forecast(gdp.1.ets, h = 3)
gdp.1.ses
round(accuracy(gdp.1.ets),3)
round(accuracy(gdp.1.ses),3)
# Holts Linear Trend Model 2 (level + trend)-
gdp.2.holt <- holt(gdp, h = 3)
gdp.2.holt
round(accuracy(gdp.2.holt),3)
autoplot(gdp.2.holt)
autoplot(gdp.2.holt,) + autolayer(fitted(gdp.2.holt), series = "Fitted")
gdp.2.holt
gdp.2.ets <- ets(gdp,model = "AAN")
gdp.2.ets
forecast(gdp.2.ets, h = 3)
round(accuracy(gdp.2.ets),3)
# Holts Model with damped Trend Model 3 (level + trend + damped)-
gdp.3.holt.d <- holt(gdp, h = 3, damped = TRUE, PI=FALSE)
gdp.3.holt.d
round(accuracy(gdp.3.holt.d),3)
autoplot(gdp.3.holt.d)
autoplot(gdp.3.holt.d,) + autolayer(fitted(gdp.3.holt.d), series = "Fitted")
gdp.3.holt.d
round(accuracy(gdp.3.holt.d),3)
# ETS method Model
gdp.ets <- ets(gdp, model = "ZZZ")
forecast(gdp.ets, h = 3)
round(accuracy(gdp.ets),3)
gdp.ets
# ETS method Model
gdp.ets <- ets(gdp, model = "ZZZ")
gdp.ets
forecast(gdp.ets, h = 3)
round(accuracy(gdp.ets),3)
autoplot(gdp.ets,) + autolayer(fitted(gdp.ets), series = "Fitted")
autoplot(gdp.ets)
autoplot(gdp.ets) + autolayer(fitted(gdp.ets), series = "Fitted")
install.packages("tseries")
install.packages("tseries")
# checking stationarity
adf.test(gdp)
adf.test(gdp)
# checking stationarity
library(tseries)
adf.test(gdp)
library(fpp2)
# checking best number of difference in time series
ndiffs(gdp)
autoplot(gdp)
# diff = 1
dgdp <- diff(gdp)
autoplot(dgdp)
# checking stationarity
library(tseries)
adf.test(dgdp) # not stationary
# diff = 2
ddgdp <- diff(gdp, lag = 2)
# diff = 2
ddgdp <- diff(gdp, lag = 2)
autoplot(ddgdp)
# checking stationarity
adf.test(ddgdp) # not stationary
# diff = 1
dgdp <- diff(gdp)
autoplot(dgdp)
# checking stationarity
library(tseries)
adf.test(dgdp) # not stationary
# diff = 2
ddgdp <- diff(gdp, lag = 2)
autoplot(ddgdp)
# checking stationarity
adf.test(ddgdp) # not stationary
# Chossing p and q value
ggtsdisplay(ddgdp)
ddgdp.1 <- arima(x = gdp, order = c(1,2,0))
ddgdp.1
round(accuracy(ddgdp.1),3)
autoplot(gdp.ets) + autolayer(fitted(ddgdp.1), series = "Fitted")
autoplot(ddgdp.1) + autolayer(fitted(ddgdp.1), series = "Fitted")
# Chossing p and q value
ggtsdisplay(ddgdp)
# Model 2
ddgdp.2 <- arima(x = gdp, order = c(4,2,0))
ddgdp.2
round(accuracy(ddgdp.2),3)
# Checking auto ARIMA
ddgdp.auto <- auto.arima(gdp)
# Checking auto ARIMA
ddgdp.3.auto <- auto.arima(gdp)
ddgdp.3.auto
round(accuracy(ddgdp.3.auto),3)
# Evaluating model
qqnorm(ddgdp.2$residuals)
qqline(ddgdp.2$residuals)
Box.test(x= ddgdp.2$residuals, type="Ljung-Box")
# Evaluating model
qqnorm(ddgdp.3.auto$residuals)
qqline(ddgdp.3.auto$residuals)
Box.test(x= ddgdp.3.auto$residuals, type="Ljung-Box")
ddgdp.2
ddgdp.3.auto
ddgdp.2
round(accuracy(ddgdp.2),3) #####-------BEST---------############
ddgdp.1
round(accuracy(ddgdp.1),3)
ddgdp.2
round(accuracy(ddgdp.2),3) #####-------BEST---------############
gdp.2.ets
forecast(gdp.2.ets, h = 3)
round(accuracy(gdp.2.ets),3)
# Evaluating model
qqnorm(gdp.2.ets$residuals)
qqline(gdp.2.ets$residuals)
Box.test(x= gdp.2.ets$residuals, type="Ljung-Box")
# Evaluating model
qqnorm(ddgdp.2$residuals,main = "Arima (p,d,q) = (4,2,0)")
qqline(ddgdp.2$residuals)
Box.test(x= ddgdp.2$residuals, type="Ljung-Box")
remove(list = ls())
# reading the gdp file
data <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Statistics for Data Analytics/Lab/Project/Time-Series/GDP_Ireland.csv")
str(data)
summary(data)
# converting the predictor to billion figures
data$Gross.Domestic.Product..GDP. <- data$Gross.Domestic.Product..GDP./1000000000
summary(data)
# sorting the data
data <- data[order(data$Year),]
row.names(data) <- 1:49
# creating time series data and analysing
gdp <- ts(data = data$Gross.Domestic.Product..GDP., start = 1970, end = 2018, frequency = 1)
par(mfrow = c(2,2))
# checking the plots - examining the error, trend, seasonality
plot(gdp, main = "Normal")
library(fpp2)
# smotthing the plot for checking Moving average
plot(ma(gdp,3), main = "q = 3")
plot(ma(gdp,5), main = "q = 5")
plot(ma(gdp,7), main = "q = 7")
# Models
# Simple Exponential Model 1 (Only level)-
par(mfrow = c(1,1))
# Models Expential Smoothing
par(mfrow = c(1,1))
# Holts Linear Trend Model 2 (level + trend)-
gdp.2.ets <- ets(gdp,model = "AAN")
gdp.2.ets
round(accuracy(gdp.2.ets),3)
# Holts Model with damped Trend Model 3 (level + trend + damped)-
gdp.3.holt.d <- holt(gdp, h = 3, damped = TRUE, PI=FALSE)
gdp.3.holt.d
round(accuracy(gdp.3.holt.d),3)
gdp.ets
round(accuracy(gdp.ets),3)
# Auto ETS method Model
gdp.ets <- ets(gdp, model = "ZZZ")
gdp.ets
round(accuracy(gdp.ets),3)
autoplot(gdp)
par(mfrow = c(2,2))
# checking the plots - examining the error, trend, seasonality
autoplot(gdp)(gdp, main = "Normal")
library(fpp2)
# smotthing the plot for checking Moving average
autoplot(gdp)(ma(gdp,3), main = "q = 3")
# checking the plots - examining the error, trend, seasonality
autoplot(gdp)(gdp, main = "Normal")
autoplot(gdp)
# checking best number of difference in time series
ndiffs(gdp) # 2
# diff = 1
dgdp <- diff(gdp)
autoplot(dgdp)
# checking stationarity
library(tseries)
adf.test(dgdp) # not stationary
# diff = 2
ddgdp <- diff(gdp, lag = 2)
autoplot(ddgdp)
# checking stationarity
adf.test(ddgdp) # stationary
# Chossing p and q value
ggtsdisplay(ddgdp)
# Model 1
ddgdp.1 <- arima(x = gdp, order = c(1,2,0))
ddgdp.1
round(accuracy(ddgdp.1),3)
# Model 2
ddgdp.2 <- arima(x = gdp, order = c(4,2,0))
ddgdp.2
round(accuracy(ddgdp.2),3) #####-------BEST---------############
# Checking auto ARIMA
ddgdp.3.auto <- auto.arima(gdp)
ddgdp.3.auto
round(accuracy(ddgdp.3.auto),3)
# Evaluating model
qqnorm(ddgdp.2$residuals,main = "Arima (p,d,q) = (4,2,0)")
qqline(ddgdp.2$residuals)
Box.test(x= ddgdp.2$residuals, type="Ljung-Box")
# Evaluating model
par(mfrow = c(2,2))
# Evaluating model
par(mfrow = c(1,1))
qqnorm(ddgdp.2$residuals,main = "Arima (p,d,q) = (4,2,0)")
qqline(ddgdp.2$residuals)
checkresiduals(ddgdp.2)
qqnorm(ddgdp.2$residuals,main = "Arima (p,d,q) = (4,2,0)")
qqline(ddgdp.2$residuals)
checkresiduals(ddgdp.2)
round(accuracy(gdp.2.ets),3)
round(accuracy(gdp.3.holt.d),3) ######------BEST ETS-----######
round(accuracy(gdp.ets),3)
round(accuracy(ddgdp.1),3)
# Chossing p and q value
ggtsdisplay(ddgdp)
ddgdp.2
round(accuracy(ddgdp.2),3) #####-------BEST---------############
ddgdp.1
ddgdp.3.auto
qqnorm(ddgdp.2$residuals,main = "Arima (p,d,q) = (4,2,0)")
qqline(ddgdp.2$residuals)
